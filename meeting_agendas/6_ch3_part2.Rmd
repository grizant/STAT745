---
title: "Methods for Generating Random Variables (Ch. 3)"
subtitle: "Part 2"
author: "AG Schissler"
date: "11 Feb 2021 (*updated: `r Sys.Date()`)*"
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
---

```{r LoadLib, echo = FALSE, eval = TRUE, message=FALSE, warning=FALSE, results = "hide"}
set.seed( 02092021 )
library(tidyverse)
library(printr)
## xaringan::inf_mr('5_MarkovChains_ch3.Rmd')
## str(knitr::opts_chunk$get())
```

<!-- open up etext as you go. don't typeset the chapter -->

# i. Admin & Startup

## schedule review / Problem Set 1 due Friday

- Let's review the schedule and place today's learning in context.  
- Please complete Problem Set 1, due Friday. 
- Call `set.seed(02122021)` before any random number generation and do not set other seeds.

## Today's plan

- Discuss Ch. 3 Problem Set
- Finish Ch. 3 discussion

*Please contribute questions and comments*

## Ch. 3 Problem Set

- Please complete Rizzo exercises 3.1, 3.3, 3.5, 3.7, 3.8, 3.12, 3.13, 3.15, 3.18.
- Submit a html report with the rmd source.
- Call set.seed( 02192021 ) before the 1st random number generator call.
- If the problem does explicitly mention the sample size use 1000.

## Review last meeting via interpreting exercises

- Let's discuss exercise prompts on p. 96 to review last meeting content.
- Recall that Rizzo provides code.

# I. 3.4 Transformation methods

## 3.4 Transformation methods

- Recall all the relationships between probability distributions.
- Those relationships can be leveraged in rv generation.
- Let's briefly discuss some examples.

# II. 3.5 Sums and Mixtures

## Convolutions

- p. 75
- example 3.10

## Mixtures

- p. 77. terms and notation
- Discrete and continuous.
- Convolution vs mixture (Figure 3.3)
- Compare Figures 3.4 and 3.5

## Example 3.15 Possion-Gamma 

<!-- mention T-Poisson paper -->

```{r ex315}
n <- 1000
r <- 4
beta <- 3
lambda <- rgamma(n, r, beta) #lambda is random
x <- rpois(n, lambda)        #the mixture
mix <- tabulate(x+1) / n
negbin <- round(dnbinom(0:max(x), r, beta/(1+beta)), 3)
se <- sqrt(negbin * (1 - negbin) / n)
```

## Example 3.15 Possion-Gamma 

```{r ex315res}
round(rbind(mix, negbin, se), 3)
```

# III. 3.6 Multivariate distributions

## Generating Multivariate normal

- variance-covariance matrix $\Sigma$ is a (square) symmetric positive definite matrix, giving rise to many computationally convenient factorizations.
- spectral decomposition
- Cholesky factorization
- SVD (= spectral for $\Sigma$ but slower as is for rectangular matrix)

## Example 3.18 Cholesky

```{r ex318function}
rmvn.Choleski <-
  function(n, mu, Sigma) {
    # generate n random vectors from MVN(mu, Sigma)
    # dimension is inferred from mu and Sigma
    d <- length(mu)
    Q <- chol(Sigma) # Choleski factorization of Sigma
    Z <- matrix(rnorm(n*d), nrow=n, ncol=d)
    X <- Z %*% Q + matrix(mu, n, d, byrow=TRUE)
    X
  }
```

##  Example 3.18 Cholesky (cond)

```{r ex318iris}
y <- subset(x=iris, Species=="virginica")[, 1:4]
mu <- colMeans(y)
Sigma <- cov(y)
mu
Sigma
```

##  Example 3.18 Cholesky (cond)

```{r ex318plot}
#now generate MVN data with this mean and covariance
X <- rmvn.Choleski(200, mu, Sigma)
pairs(X)
```

## Discuss Remark 3.3

- Data "whitening"  via transformations to remove correlations 

## Basic benchmarking

- use of `system.time()` p. 89-90

## Mixtures of MNV

- highly flexible and useful to test robust statistical methods (robust to non-normality)
- Discuss Figure 3.8

## 3.6.3 Wishart distribution

- p. 92-93
- Inverse Wishart is important for Bayesian inference for covariance
- see [https://en.wikipedia.org/wiki/Inverse-Wishart_distribution#Conjugate_distribution](https://en.wikipedia.org/wiki/Inverse-Wishart_distribution#Conjugate_distribution)

## 3.6.4 Uniform distribution

- Important for higher dimensional probability:
- Standard MVN at high dimensions has almost all probability on sphere of radius $\sqrt d$!
- this is completely counter to our intuition trained at lower dimensions.

For more: 

Vershynin, Roman. *High-dimensional probability: An introduction with applications in data science*. Vol. 47. Cambridge university press, 2018.

# Closing

- Briefly discuss remaing exercise prompts.
- Students please summarize/ask questions about what we discussed today.
- Other questions?
