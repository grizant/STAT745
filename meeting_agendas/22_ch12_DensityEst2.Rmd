---
title: "PROBABILITY DENSITY ESTIMATION"
subtitle: "Chapter 12 SCR2e, Part 2"
author: "AG Schissler"
date: "15 Apr 2021 (*updated: `r Sys.Date()`)*"
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
---

```{r LoadLib, echo = FALSE, eval = TRUE, message=FALSE, warning=FALSE, results = "hide"}
set.seed( 04152021 )
library(tidyverse)
library(printr)
## xaringan::inf_mr('22_ch12_DensityEst2.Rmd')
## str(knitr::opts_chunk$get())
```

<!-- open up etext as you go. don't typeset the chapter -->

# i. Week 12 Focus

- Students Practice on Ch. 11 content.
- Students receive and act on Ch. 10 feedback.
- Read Prob Den. Est. Ch. 12.
- Discuss Prob Den. Est.  Ch. 12.

## Today's plan

i. Discuss Ch.10 HW feedback
ii. Discuss Ch.11 HW 
iii. Prime learning by previewing Ch.12 HW.
iv. Discuss Ch. 12 Prob Den. Est. part 1 (meeting 21)
v. Discuss Ch. 12 Prob Den. Est. part 2
vi. Summarize, conclude, revisit Ch. 12 Problem prompt discussion / hints time remaining

*Please contribute questions and comments*

## Ch.10 HW feedback

- ???

## Any questions on Ch.11 HW?

- 11.1, 11.2 M-H practice and understanding.
- 11.5 MCMC in Bayesian inference.
- 11.10 Gibbs with full conditionals available. (Please use $a=2,b=3,n=10$ for standardization answers).
- 11.12 Convergence diagnostics
- ~~11.15 Change Point and HPDI.~~

# I. Ch.12 HW

- 12.1 Histogram estimation (hint try `histObject$density`).
- 12.4 Frequency polygon density (try `approxfun()` to check your work with `integrate`)
- 12.6 ASH density for old faithful (hint: estimate (12.9), see Example 12.6).
- 12.8 Kernel density for `buffalo` snowfall data (just use `density` apprpopriately. See Table 12.2 for insight into whether binwidth or choice of kernel is more important).
- 12.13 Generalize 2D ASH. (now BONUS; this is a theoretical question to first develop the approach and then it would be good to apply to simulated/real data).
- May find `nclass.Sturges` helpful or binning rules from `nclass.XXX`.

# Ch. 12 Probability Density Estimation

## Probability Density Estimation, Part 2

* 12.2 Kernel Density Estimation
  * Example 12.7 - 12.9
  * Boundary kernels, Examples 12.10-12.11
* 12.3 Bivariate/Multivariate Den. Est.
  * 12.3.1 Bivariate Frequency Polygon
	* Example 12.12 - 12.13
  * 3D Histogram
  * 12.3.2 Bivariate ASH
	* Example 12.14 / Fig 12.12
  * 12.3.3 Multidimensional Kernel Methods
	* Example 12.15 / Fig 12.13
* 12.4 Other methods in Den. Est.

# 12.2 Kernel Density Estimation

## Kernel Density Estimation overview

- Generalizes idea of histogram-based density estimation
- Recall (12.1): $\hat{f}(x) = \frac{\nu_k}{nh}$, for $x$ in class with boundaries $t_k \leq x < t_{k+1}$.
- Centering the class interval on midpoint $x$ with binwidth $2h$ gives $\hat{f}(x) = \frac{k}{2hn}$, $x \in (x-h, x+h)$, where $k$ in the number of sample points in the interval.
- This can be written more generally as (12.11): $\hat{f}(x) = \frac{1}{n} \sum_{i=1}^n \frac{1}{h} w \left( \frac{x - X_i}{h} \right)$.
- $w(\cdot)$ is called the $weight function$.
- Equation (12.1) is called the *naive density estimator* and has weight function $w(t)=\frac{1}{2} I(|t|<1)$.
- This is a probability distribution on $[-1,1]$! (it is Uniform(-1,1) / rectangular shape / symmetric).
- *Kernel density estimation* replaces $w()$ with a kernel function $K(\cdot)$ is a density, usually symmetric.
- Triangular kernel (ASH converges to this). Or Gaussian kernel.

## Kernel Density Estimation overview

- Properties of the kernel propagate to the estimator.
- If $K()$ is continuous/smooth $\hat{f}()$ will be also.
- Smaller binwidth $h$ will be more responsive to data points (local features).
- Larger binwidth $h$ will reveal global (regular) features. (see Figure 12.6)
- See Figure 12.7 and Table 12.2.
- Silverman's rule is a good default choice. See `bw.nrd`.

## Example 12.7 / Fig 12.8

- etext

## Example 12.8 / Fig 12.9

- etext

## Example 12.9

- etext

## Boundary kernels

- Reflection boundary technique (draw on ipad if needed)
- Example 12.10 
- Example 12.11
- Fig 12.10

# 12.3 Bivariate/Multivariate Den. Est.

## 12.3.1 Bivariate Frequency Polygon
	
## Example 12.12

## Example 12.13 / Fig 12.11

## 3D Histogram

## 12.3.2 Bivariate ASH

## Example 12.14 / Fig 12.12

## 12.3.3 Multidimensional Kernel Methods

## Example 12.15 / Fig 12.13

# 12.4 Other methods in Den. Est.

- Kullback-Liebler divergence/distance is powerful, information theoretic based loss function.

# Closing / Ch. 12 Problem set

- Revisit HW.
- Questions?
- Students please summarize/ask questions about what we discussed today.
