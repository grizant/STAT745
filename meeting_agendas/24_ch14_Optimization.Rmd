---
title: "OPTIMIZATION"
subtitle: "Chapter 14 SCR2e"
author: "AG Schissler"
date: "22 Apr 2021 (*updated: `r Sys.Date()`)*"
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
---

```{r LoadLib, echo = FALSE, eval = TRUE, message=FALSE, warning=FALSE, results = "hide"}
set.seed( 04222021 )
library(tidyverse)
library(printr)
## xaringan::inf_mr('24_ch14_Optimization.Rmd')
## str(knitr::opts_chunk$get())
```

<!-- open up etext as you go. don't typeset the chapter -->

# i. Week 13 Focus

- Students begin work on Final Exam. Let me know what you plan to do.
- Students Practice on Ch.12 content.
- Students receive and act on Ch. 11 feedback.
- Read Numerical Methods in `R` Ch. 13
- Read Optimization Ch. 14
- Discuss Ch. 13, 14

## Any questions on Ch.12 HW?

- Discussion: which "normal refererence rule" to use?

- 12.1 Histogram estimation
- 12.4 Frequency polygon density
- 12.6 ASH density for old faithful
- 12.8 Kernel density for `buffalo` snowfall data.
- 12.13 Generalize 2D ASH (BONUS NOW)

## Today's plan

i. Conclude Ch.13 Numerical Methods in R
ii. Open with questions on Ch.14 Problem Set to prime learning.
iii. Discuss Ch.14 Optimization
iv. Summarize, conclude, revisit Ch.14 Problem prompt discussion / hints time remaining

*Please contribute questions and comments*

# Ch.13 con'd

## Revisit Example 13.3

```{r}
k <- 100
    system.time({
        for (i in 1:1000) {
            a <- rep(0, 24); a0 <- pi / 6; a2 <- a0 * a0
            a[1] <- -a0^3 / 6
            for (i in 2:k)
                a[i] <- - a2 * a[i-1] / ((2*i+1)*(2*i))
            a0 + sum(a)} })
    system.time({
        for (i in 1:1000) {
            K <- 2 * (0:k) + 1
            i <- rep(c(1, -1), length=(k+1))
            sum(i * (pi/6)^K / factorial(K))}
        })
```

## 13.2 Root-finding

- WLOG we can focus on solving $f(x)=0$.
- Two classes: first derivative needed or not.
- In any event you must provide the function two endpoints where $f(\cdot)$ changing sign.
- Brent's method explained briefly
- Example 13.6
- Example 13.7

## Ch.13,14 HW Preview/Hints

*Let me clarify my advice for 13.4, 13.5*

- 13.3 Efficient implementation a high dimension, computationally intense procedure.
- 13.4 Computing critical values for a nonstandard statistical procedure. Hint: work with the difference. Plot the difference to find an interval that changes see. Then use `uniroot`.to find 1D root.  Note the function is well defined for $|a|=k$. Final hint: Use `pt()`.
- 13.5 Computing critical values for a nonstandard statistical procedure II. Hint: Recognize that `a(k)` is the root from 13.4. Also please compute and report `c_k`.
- 14.1 Use the *simplex* algorithm to solve a small system of equations. Use `boot::simplex()`.

# Ch.14 Optimization

*Focus is on implementation, not theory*.

- 14.1 Introduction
- 14.2 One-dimensional Optimization
- 14.3 Maximum Likelihood Estimation with `mle`
- 14.4 Two-dimensional Optimization
- 14.5 The EM Algorithm
- 14.6 Linear Programming --- The Simplex Method

## 14.1 Introduction

- Goal
- R functions for optimization: `optimize`, `optim`, `constrOptim`, `nlm`, `mle`.
- List of methods

## 14.2 One-dimensional Optimization

- Newton algorithms, golden search, interpolations
- Example 14.1 1D optimization via `opimize`

## 14.3 Maximum Likelihood Estimation with `mle`

- `mle` wraps `optim`
- Discuss `...` for wrapping functions.
- Example 14.2 MLE: Gamma distr.

## 14.4 Two-dimensional Optimization

- `optim` methods: Nelder-Mead, Quasi-Newton, Conjugate-gradient algorithms
- Example 14.3 2D via `optim`
- Example 14.4 MLE for a quadratic form

## 14.5 The EM Algorithm

- Expectation-Maximization (EM) algorithm importance/use
- Example 14.5 EM for a mixture model

## 14.6 Linear Programming --- The Simplex Method

- Method for certain types of constrained optimization
- Example 14.6 Simplex algorithm

# Closing / Ch.13 Problem set

- Revisit HW.
- Questions?
- Students please summarize/ask questions about what we discussed today.
