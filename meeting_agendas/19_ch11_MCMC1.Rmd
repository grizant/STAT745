---
title: "MARKOV CHAIN MONTE CARLO METHODS"
subtitle: "Chapter 11 SCR2e, Part 1"
author: "AG Schissler"
date: "6 Apr 2021 (*updated: `r Sys.Date()`)*"
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
bibliography: '/Users/alfred/bib/library.bib'
---

```{r LoadLib, echo = FALSE, eval = TRUE, message=FALSE, warning=FALSE, results = "hide"}
set.seed( 04062021 )
library(tidyverse)
library(printr)
## xaringan::inf_mr('19_ch11_MCMC1.Rmd')
## str(knitr::opts_chunk$get())
```

<!-- open up etext as you go. don't typeset the chapter -->

# i. Week 11 Focus

- Students Practice on Ch. 10 content.
- Students receive and act on Ch. 8, 9 feedback.
- Read MCMC Ch. 11.
- Discuss MCMC Ch. 11.

## Any questions on Ch.10 HW?

- 10.1 - 10.3
- 10.6

## Today's plan

i. Open with questions on Ch. 11 Problem Set to prime learning.
ii. Discuss Ch. 11 MCMC part 1
iii. Summarize, conclude, revisit Ch. 11 Problem prompt discussion / hints time remaining

*Please contribute questions and comments*

# Ch. 11 MCMC

- 11.1 Introduction: MCMC / Bayes inference
- 11.2 The Metropolis-Hastings Algorithm
- 11.3 The Gibbs Sampler
- 11.4 Monitoring Convergence
~~- 11.5 Change Point analysis~~

# I. Ch.11 HW

- 11.1, 11.2 M-H practice and understanding.
- 11.5 MCMC in Bayesian inference.
- 11.10 Gibbs with full conditionals available.
- 11.12 Convergence diagnostics
~~- 11.15 Change Point and HPDI.~~

# 11.1 Introduction: MCMC 

- Markov chain defintions
- Basisc Markov chain example
- MCMC integration / Bayesian inference

## Markov chain definitions

Wait, what _is_ a Markov Chain?

**Def'n 2.A**: A Markov Chain is a stochastic process $\{ X_t \}$ indxed over $t \geq 0$ that exists on a _state space_ $\chi$ such that if $\chi$ is the space $\{ 0,1,2,\ldots \}$ (special case: countable for simplicity), then $p_{ij}=P(X_{t+1} = j | X_t = i)=P(X_{t+1} = j | X_t = i, X_{t-1} = i_{t-1}, X_{t-2} = i_{t-2}, \ldots, X_1 = i_1, X_0 = i_0)$.

I.e., the *transition probability* of moving from state $i$ to state $j$ depends only on the current state $i$ (and not the history). They are commonly used to model systems with short memories, such as the stock market. 

Let the *transition matrix* be ${\bf P} = \{ p_{ij} \}$ and consider a finite state space. Denote $p^{(k)}_{ij} = P( X_t \: \mathrm{goes \: from} \: i \: \mathrm{to} \: j \: \mathrm{in} \: k \: \mathrm{steps})$. One can show (the "Chapman-Kolmogorov Equations") that ${\bf P}^{(k)}=p^{(k)}_{ij}={\bf P}^k$ (just take powers of ${\bf P}$).

## Markov chain definitions

**Def'n 2.B**: A Markov Chain is *irreducible* if $X_t$ can move from state $i$ to $j$ in finite time with positive probability.

**Def'n 2.C**: A state $i$ is *recurrent* if the chain $\{ X_t \}$ returns to $i$ with probability 1. (non-recurrent states are called *transient*.) If the expected time to return to $i$ is finite, the state is *positive recurrent*.

**Def'n 2.D**: The *period* of a state, $i$, is the greatest common divisor of the lengths of paths starting and ending at $i$.

**Def'n 2.E**:  A chain is *aperiodic* if all states have period 1.

**Def'n 2.F**: A positive recurrent, aperiodic state is *ergodic* (if all states are in the chain are ergodic, the chain is said to be erogic).

## Markov chain definitions

**Def'n 2.G**: The *stationary distribution* of a finite-state, irreducible, aperiodic Markov Chain is the probability distribution:

$$
{\bf \pi} = \{ \pi_j \} \: \mathrm{satisfying} \: \pi_j = \lim_{n \to \infty} p^{(n)}_{ij} \ni \\
\pi_j = \sum^{\infty}_{i=0} \pi_i p_{ij} \: (j \geq 0) \: \mathrm{and}  \sum^{\infty}_{i=0} \pi_j = 1.
$$

View the statitionary distribution $\{ \pi_j \}$ as the (limiting) proportion of time that the chain rests in state $j$.

## Basic Markov Chain example

From @Braun2016, consider a disease model with 3 stages. Stage 1 is healthy, Stage 2 is mild disease, and Stage 3 is severe disease. Healthy individuals remain healthy with probability 0.99 and develop mild disease with prob 0.01. Individuals with mild disease become healthy with prob 0.5, stay mild with prob 0.4, and become severely ill with prob 0.1. Finally those with severe disease stay severely sick with prob 0.75 and progress to mild status with prob 0.25.

This setting describes a Markov chain with $n=3$ states. The transition matrix is given by

$$
  P =
  \left[ {\begin{array}{ccc}
   0.99 & 0.01 & 0.00 \\
   0.50 & 0.40 & 0.10 \\
   0.00 & 0.25 & 0.75 \\
  \end{array} } \right]
$$

We'll simulate two individuals for 10000 steps: one who starts healthy and one who begins with severe disease.

## Basic Markov Chain example

```{r ex1}
simreps <- 10000
P <- matrix(data = c(0.99, 0.01, 0, 0.5, 0.4, 0.1, 0, 0.25, 0.75), nrow = 3, byrow = TRUE)
n <- nrow(P)
X <- numeric(simreps)
```

## Basic Markov Chain example

```{r ex1b}
## for a healthy person at time 1
X[1] <- 1

for (t in 1:(simreps-1)) {
    X[t + 1] <- sample(x = 1:n, size = 1, prob = P[X[t], ])
}

cat("Healthy person to start")
table(X) / length(X)
```

## Basic Markov Chain example

```{r ex1c}
## for a severly ill person at time 1
X[1] <- 3

for (t in 1:(simreps-1)) {
    X[t + 1] <- sample(x = 1:n, size = 1, prob = P[X[t], ])
}

cat("Serevely ill person to start")
table(X) / length(X)
```

## MCMC integration 

- We've used MC methods to integrate (so we can compute probabilities, moments, etc)
- For this to work, we need a to be able to sample for a target density $f(x)$.
- The MCMC approach is to construct a Markov Chain with stationary distribution $f(x)$ and run the chain long enough to converge (approx.) to the stationary distribution.
- With these nearly independent pseudo-samples in hand, MC methods are available to integrate or other types of statistical inference.
- A generalization of the strong law of large numbers guarentees the sample mean $\bar{g(X)_m}$ converges to $E[g(X)]$ as $m \to \infty$ (see p.299 for details).

## MCMC Bayesian inference

- The MCMC approach is commonly used to conduct Bayesian calculations and inference.
- Let's label the components of Bayes' rule as the *prior distribution*, *likelihood*, and *marginal likelihood*.
- Note the joint likelihood $f_{(X|\theta)}(x)$ from a iid sample of size $n$ is shorthand for $\prod_{i=1}^{n} f_{(X|\theta)}(x_i | \theta)$.
- Note the denominator in Eqn (11.1) is the marginal likelihood $f_X(x)$. Approximation of this integral is the main motivation behind using MCMC for Bayesian inference, especially in high dimensions. High-dimensional integration gets hard fast.
- Small typo in Eqn (11.2). Leftmost expression should be $E[ g(\theta) | x ]$.

# 11.2 The Metropolis-Hastings Algorithm (1970)

The Tale of King Markov from R. McElreath's *Statistical Rethinking*

[McElreath's slides](https://speakerdeck.com/rmcelreath/statistical-rethinking-fall-2017-lecture-10)

## 11.2.1 Metropolis-Hastings sampler

The *Metropolis-Hastings sampler* generates a Markov chain $\{X_0, X_1, \ldots \}$ as follows:

1. Choose a proposal distribution $g(\cdot | X_t)$, subject to regularity conditions discussed below.  
2. Generate $X_0$ from distribution $g$.  
3. Repeat (until chain has convergened to a stationary distribution):  
	(a) Generate $Y$ from $g(\cdot | X_t)$.  
	(b) Generate $U$ from Uniform(0,1).
	(c) If  
	$$
	U \leq \frac{f(Y)g(X_t|Y)}{f(X_t)g(Y|X_t)}
	$$
accept $Y$ and set $X_{t+1} = Y$; otherwise set $X_{t+1} = X_t$.  
	(d) Increment $t$.

## Metropolis-Hastings sampler

Observe that in step (3c) the candidate point $Y$ is accepted with probability
	$$
	\alpha (X_t, Y) = min \left( 1, \frac{f(Y)g(X_t|Y)}{f(X_t)g(Y|X_t)} \right),
	$$

 So it is only necessary to know the density of the target distribution $f$ up to a constant

**The algorithm is designed so that the stationary distribution of the chain is $f$.**

The proposal distribution must be chosen so that the generated chain will converge to a stationary distribution --- the target distribution. A proposal distribution with the same support as the target distribution will generally satisify regularity conditions (irreducibility, positive recurrence, and aperiodicity) to guarantee convergence.

## Example 11.1 M-H for Rayleigh distribution

Let's use the M-H algorithm to generate a sample from the Rayleigh distribution. The Rayleigh distribution has density

$$
f(x) = \frac{x}{\sigma^2}e^{-x^2/(2\sigma^2)}, x \geq 0, \sigma > 0 
$$

The Rayleigh distribution is used to model lifetimes subject to rapid aging. The mode is $\sigma$, $E[X] = \sigma \sqrt{\pi / 2}$ and $Var(X) = \sigma^2 (4-\pi) /2$.

For the proposal distribution, try the chi-squared distribution with degrees of freedom of $X_t$.

## Example 11.1 M-H for Rayleigh distribution

```{r}
f <- function(x, sigma) {
    if (any(x < 0)) return(0)
    stopifnot(sigma > 0)
    return((x / sigma^2) * exp(-x^2 / (2*sigma^2)))
}
## construct the chain
m <- 10000
sigma <- 4
x <- numeric(m)
x[1] <- rchisq(1, df = 1)
k <- 0 ## rejections to investigate efficiency
u <- runif(m) ## acceptance probabilities

for (i in 2:m) {
    xt <- x[i-1]
    y <- rchisq(1, df = xt)
    num <- f(y, sigma) * dchisq(xt, df = y)
    den <- f(xt, sigma) * dchisq(y, df = xt)
    if (u[i] <= num/den) {
        x[i] <- y 
    } else {
        ## y is rejected
        x[i] <- xt
        k <- k + 1
    }
}
```

## Example 11.1 M-H for Rayleigh distribution

```{r}
print(k) / m
```

## Example 11.1 M-H for Rayleigh distribution

```{r}
## go past burn-in
index <- 2000:10000
y1 <- x[index]
plot(index, y1, type = "l", main = "Trace Plot", ylab = "x")
```

Fit looks good. 

We'd never really generate Rayleigh rvs this way. Use antithetic variates instead.

## Example 11.1 M-H for Rayleigh distribution

Check the expected value.

```{r}
## expected value
mean(y1)
## in theory
sigma * sqrt(pi /2)
```

## Bayesian inference without integrating

Notice what happens to the acceptance ratio if $f( \cdot )$ is a posterior pdf:

Derivation on iPad

- See Example 11.8 for more reading.

## 11.2.2 The Metropolis Sampler (1953)

Note that Metropolis-Hastings is a generalized of the slightly older Metropolis algorithm.

In the Metropolis algorithm, proposal distribution $g$ must be symmetric. I.e., $g(X|Y)=g(Y|X)$.

Then the acceptance ratio

$$
 r(X_t, Y) = \frac{f(Y)g(X_t|Y)}{f(X_t)g(Y|X_t)}
$$
	
simplies to 

$$
 r(X_t, Y) = \frac{f(Y)}{f(X_t)}.
$$

Why generalize (as in the Metropolis-Hastings sampler above)? Judicious choice of proposal density will provide greater efficiency (more rapid convergence to the stationary distribution) in certain cases.

@Gelman1997 recommend that the rejection rates in the range $[0.15, 0.5]$ for good performance.

## 11.2.3 Random Walk Metropolis

- Example of Metropolis sampler with (conditionally) symmetric proposal $g(Y | X_t) = g(|Y - X_t|)$
- At each iteration, a random increment $Z$ is generated from $g(\cdot)$ and $Y = X_t + Z$.
- For example, $Z \sim N(0, \sigma^2)$ leads to candidate point $Y|X_t \sim N(X_t, \sigma^2)$.
- Acceptance rates sensitive to choice of $\sigma^2$.
- As $\sigma^2 \to 0$, $Y$ will almost always accepted ($\alpha \to 1$, makes the chain look like a pure random walk.
- As $\sigma^2 \to \infty$, the points will almost always be rejected --- becoming inefficient.
- Chose scale parameters like $\sigma^2$ carefully.

## Example 11.6, 11.7 RW Metropolis

- on etext
- Discuss Figure 11.6

## 11.2.4 Independence Sampler

- Another variant of M-H is the independence sampler.
- Here the proposal is independent of the previous value of the chain: $g(Y|X_t) = g(Y)$.
- This works well when proposal and target are close, but otherwise does not. 
- Rarely used.
- See Example 11.9.

# Closing / Ch. 11 Problem set

- Revisit HW.
- Questions?
- Students please summarize/ask questions about what we discussed today.

## References
