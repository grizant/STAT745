---
title: "MARKOV CHAIN MONTE CARLO METHODS"
subtitle: "Chapter 11 SCR2e, Part 2"
author: "AG Schissler"
date: "8 Apr 2021 (*updated: `r Sys.Date()`)*"
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
bibliography: '/Users/alfred/bib/library.bib'
---

```{r LoadLib, echo = FALSE, eval = TRUE, message=FALSE, warning=FALSE, results = "hide"}
set.seed( 04082021 )
library(tidyverse)
library(printr)
## xaringan::inf_mr('20_ch11_MCMC2.Rmd')
## str(knitr::opts_chunk$get())
```

<!-- open up etext as you go. don't typeset the chapter -->

# i. Week 11 Focus

- Students Practice on Ch. 10 content.
- Students receive and act on Ch. 8, 9 feedback.
- Read MCMC Ch. 11.
- Discuss MCMC Ch. 11.

## Any questions on Ch.10 HW?

- 10.1 - 10.3
- 10.6

## Today's plan

i. Open with a discussion of Exercise 10.2-10.3, @Anderson1962.
ii. Then discuss Ch. 11 Problem Set, the difference shouldn't be that obvious for 11.1.
iii. Finish discussion Ch. 11 MCMC part 1 (pick up at Bayesian derivation)
iv. Discussion Ch. 11 MCMC part 2 
iv. Summarize, conclude, revisit Ch. 11 Problem prompt discussion / hints time remaining

*Please contribute questions and comments*

# Ch. 11 MCMC

- 11.1 Introduction: MCMC / Bayes inference
- 11.2 The Metropolis-Hastings Algorithm
- 11.3 The Gibbs Sampler
- 11.4 Monitoring Convergence
- ~~11.5 Change Point analysis~~

# I. Ch.11 HW

- 11.1, 11.2 M-H practice and understanding.
- 11.5 MCMC in Bayesian inference.
- 11.10 Gibbs with full conditionals available.
- 11.12 Convergence diagnostics
- ~~11.15 Change Point and HPDI.~~

# 11.3 The Gibbs Sampler

- Another form of M-H sampler, useful in cases with the target pdf is multi-dimensional is known as Gibbs sampling.
- The key feature in Gibbs sampling is use of the (full) conditional distributions of the target (joint) pdf.
- The univariate conditionals must be *fully* specified and easy to sample from.

## Gibbs Algorithm

- etext p.320
- Notice that the acceptance ratio = 1, manipulating $r(x_t, y)$ under this conditional structure.
- Notice the in-stream updating, one by one.

## Example 11.10 Gibbs Sampler: Bivariate dist'ns

```{r 1110}
#initialize constants and parameters
N <- 5000               #length of chain
burn <- 1000            #burn-in length
X <- matrix(0, N, 2)    #the chain, a bivariate sample
rho <- -.75; mu1 <- 0; mu2 <- 2
sigma1 <- 1; sigma2 <- .5
s1 <- sqrt(1-rho^2)*sigma1; s2 <- sqrt(1-rho^2)*sigma2
```

## Example 11.10 Gibbs Sampler: Bivariate dist'ns

```{r 1110b}
X[1, ] <- c(mu1, mu2)            #initialize
for (i in 2:N) {
    x2 <- X[i-1, 2]
    m1 <- mu1 + rho * (x2 - mu2) * sigma1/sigma2
    X[i, 1] <- rnorm(1, m1, s1)
    x1 <- X[i, 1]
    m2 <- mu2 + rho * (x1 - mu1) * sigma2/sigma1
    X[i, 2] <- rnorm(1, m2, s2)
}
b <- burn + 1
x <- X[b:N, ]
```

## Example 11.10 Gibbs Sampler: Bivariate dist'ns

```{r 1110c, warning=F}
# compare sample statistics to parameters
colMeans(x)
cov(x)
cor(x)
```

## Example 11.10 Gibbs Sampler: Bivariate dist'ns

```{r 1110d}
plot(x, main="", cex=.5, xlab=bquote(X[1]),
         ylab=bquote(X[2]), ylim=range(x[,2]))
```

## Let's watch a Gibbs sampler at work

https://chi-feng.github.io/mcmc-demo/

# 11.4 Monitoring Convergence

- Uses multiple chains to monitor convergence to the stationary distribution.
- An Analysis of Variance (ANOVA) type analysis of between-chain to within-chain variation.

## 11.4.1 Why monitor convergence

- If the proposal pdf satisfies various regularity conditions, the chain will converge to the target density $f(\cdot)$. 
- But (a) when do we know and (b) how large an $m$ or burn-in.
- Short answer we don't know.... 
- Instead, we have diagnostics and visualizations.
- Multi-chain is useful to diagnostic multi-modal issues. 4 is a good number.

## 11.4.2 Methods for monitoring

- Already seen *trace plots* and *autocorrelation function* (acf) plots
- And a quantative measure, *rejection rate*.
- And many other methods exist and are actively being developed.

## 11.4.3 The Gelman-Rubin Method

- Have you see an ANOVA before?
- Discuss p.323-325.
- Estimated *potential scale reduction* factor $\sqrt{\hat{R}}$ should be less than 1.1.

## Example 11.11 (Gelman-rubin method) `coda`

```{r 1111}
normal.chain <- function(sigma, N, X1) {
    ##generates a Metropolis chain for Normal(0,1)
    ##with Normal(X[t], sigma) proposal distribution
    ##and starting value X1
    x <- rep(0, N)
    x[1] <- X1
    u <- runif(N)
    for (i in 2:N) {
        xt <- x[i-1]
        y <- rnorm(1, xt, sigma)     #candidate point
        r1 <- dnorm(y, 0, 1) * dnorm(xt, y, sigma)
        r2 <- dnorm(xt, 0, 1) * dnorm(y, xt, sigma)
        r <- r1 / r2
        if (u[i] <= r) x[i] <- y else x[i] <- xt
    }
    return(x)
}
```

## Example 11.11 (Gelman-rubin method) `coda`

```{r 1111b}
sigma <- .2     #parameter of proposal distribution
k <- 4          #number of chains to generate
n <- 15000      #length of chains
b <- 1000       #burn-in length
##choose overdispersed initial values
x0 <- c(-10, -5, 5, 10)
```

## Example 11.11 (Gelman-rubin method) `coda`

```{r 1111c}
#generate the chains
X <- matrix(0, nrow=k, ncol=n)
for (i in 1:k)
    X[i, ] <- normal.chain(sigma, n, x0[i])
```

## Example 11.11 (Gelman-rubin method) `coda`

```{r 1111d}
library(coda)
X1 <- as.mcmc(X[1, ])
X2 <- as.mcmc(X[2, ])
X3 <- as.mcmc(X[3, ])
X4 <- as.mcmc(X[4, ])
Y <- mcmc.list(X1, X2, X3, X4)
print(gelman.diag(Y))
```

## Example 11.11 (Gelman-rubin method) `coda`

```{r 1111e}
gelman.plot(Y, col = c(1, 1))
```



## State-of-the-art MCMC

- Particle simulators including Hamiltonian Monte Carlo (HMC). 
- Watch https://chi-feng.github.io/mcmc-demo/.
- HMC has great diagnostics for monitoring convergence based on physical properties.
- Let's tour the  [`stan`](https://mc-stan.org/) website and [forums](https://discourse.mc-stan.org/).
- https://mc-stan.org/users/documentation/case-studies/rstan_workflow.html

# Closing / Ch. 11 Problem set

- Revisit HW.
- Questions?
- Students please summarize/ask questions about what we discussed today.

## References
