---
title: "MONTE CARLO METHODS IN INFERENCE"
subtitle: "Chapter 7 SCR2e, Part I"
author: "AG Schissler"
date: "16 Mar 2021 (*updated: `r Sys.Date()`)*"
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
---

```{r LoadLib, echo = FALSE, eval = TRUE, message=FALSE, warning=FALSE, results = "hide"}
set.seed( 03162021 )
library(tidyverse)
library(printr)
## xaringan::inf_mr('13_ch7_MCinference1.Rmd')
## str(knitr::opts_chunk$get())
```

<!-- open up etext as you go. don't typeset the chapter -->

# i. Admin & Startup

- review schedule

## Action items

### Students

- Ch. 6 HW due 19 Mar 2021.
- Ch. 7 HW due 26 Mar 2021.

### Instructor

- Grade Ch. 5 HW
- Prep meeting agendas for this week: Ch.7.
- Prep meeting agendas for next week: Ch.8.

## Any questions on Ch.6 HW?

- 6.1 (MC integration; see example 6.1,6.2)
- 6.2 (MC `pnorm`; see example 6.3)
- 6.3 (comparing MC estimators)
- 6.6, 6.7 (antithetic variates)
- 6.12 (importance sampling theory; see p.172)
- 6.13 (importance sampling; focus on the support; hint use translation)
- 6.15 (stratified importance sampling; see examples 6.11 and 6.14)

## Today's plan

i. Open with questions on Ch. 7 Problem Set to prime learning.
ii. Discuss roughly half of Ch. 7
iii. Summarize, conclude, revisit Ch. 7 Problem prompt discussion / hints time remaining

*Please contribute questions and comments*

# MC for Inference Overview

- 7.1 Introduction
- 7.2 MC Methods for Estimation
- 7.3 MC Methods for Hypothesis Testing 
- 7.4 Application: "Count Five" Test for Variance

# I. Ch.7 HW

- 7.1 Trimmed mean
- 7.3 Power of $t-test$
- 7.5 Run length encoding
- 7.6 robustness of $t-interval$
- 7.10 Gini index 
- Project 7.D. (mvn tests of normality ; see `mlbench`package)

# II. 7.1 Introduction

## Example estimation tasks 

- estimate parameters of sampling distribution of a statistic
- Estimate MSE
- quantiles, probabilities, etc.

## Example inferential tasks 

- Coverage rates for confidence intervals
- Empirical Type I error rates
- Empirical Power/Type II error rates 
- Make methodological comparisons 

## Categorization of MC methods

- Ch. 7 methods draw random samples from a given model, sometimes called *parameteric* bootstrap.
- This is to constrast to the popular, nonparametric bootstrap that will be discussed in Ch. 8.
- Bootstrap is a type of a *resampling* method, whereas Ch. 7 is just sampling.

# III. 7.2 MC Estimation

## Discuss 7.2 notation

- Any questions on the notation on p.184?

## 7.2.1 MC Estimation and SE

### Example 7.1

```{r 71}
m <- 1000
g <- numeric(m)
for (i in 1:m) {
    x <- rnorm(2)
    g[i] <- abs(x[1] - x[2])
}
est <- mean(g)
est
```

## Estimating the SE of the mean

- Discuss formulas and the notion of "plug-in"

```{r 71b}
sqrt( sum(( g - mean(g))^2 )) / m
0.02695850 ## exact
```

## 7.2.2 Estimation of MSE

- Discuss formulas on p.185-186
- Algorithm on p. 186.

## Example 7.2 trimmed mean

```{r 72}
n <- 20
m <- 1000
tmean <- numeric(m)
for (i in 1:m) {
    x <- sort(rnorm(n))
    tmean[i] <- sum(x[2:(n-1)]) / (n-2)
}
mse <- mean(tmean^2)
mse
sqrt(sum((tmean - mean(tmean))^2)) / m    #se
```

## Example 7.2 trimmed mean (con'd)

```{r 72b}
n <- 20
m <- 1000
tmean <- numeric(m)
for (i in 1:m) {
    x <- sort(rnorm(n))
    tmean[i] <- median(x)
}
mse <- mean(tmean^2)
mse
sqrt(sum((tmean - mean(tmean))^2)) / m    #se
```

## Example 7.3 (MSE of a trimmed mean, cont.)

```{r 73}
set.seed(522)
n <- 20
K <- n/2 - 1
m <- 1000
mse <- matrix(0, n/2, 6)

trimmed.mse <- function(n, m, k, p) {
## MC est of mse for k-level trimmed mean of
## contaminated normal pN(0,1) + (1-p)N(0,100)
    tmean <- numeric(m)
    for (i in 1:m) {
        sigma <- sample(c(1, 10), size = n,
                        replace = TRUE, prob = c(p, 1-p))
        x <- sort(rnorm(n, 0, sigma))
        tmean[i] <- sum(x[(k+1):(n-k)]) / (n-2*k)
    }
    mse.est <- mean(tmean^2)
    se.mse <- sqrt(mean((tmean-mean(tmean))^2)) / sqrt(m)
    return(c(mse.est, se.mse))
}
```

## Example 7.3, Table 7.1

```{r 73b, warning=F}
for (k in 0:K) {
    mse[k+1, 1:2] <- trimmed.mse(n=n, m=m, k=k, p=1.0)
    mse[k+1, 3:4] <- trimmed.mse(n=n, m=m, k=k, p=.95)
    mse[k+1, 5:6] <- trimmed.mse(n=n, m=m, k=k, p=.9)
}
head(n*mse)
```

## 7.2.3 Estimating a confidence level

- Simulation used to confirm theory.
- E.g., simulate from the model and estimate the quantiles commonly-used to construct CI
- Or to test the model robustness to departures from assumptions.
- As in Example 7.4, the fact the CI for variance is sensitive to mild departures from normality.

## Example 7.4 (Confidence interval for variance)

```{r 74}
n <- 20
alpha <- .05
x <- rnorm(n, mean=0, sd=2)
(UCL <- (n-1) * var(x) / qchisq(alpha, df=n-1))
```

## MC experiment to estimate a confidence level

- Discuss algorithm on p.190.

## Example 7.5 (MC estimate of confidence level)

```{r 75}
n <- 20
alpha <- .05
UCL <- replicate(1000, expr = {
    x <- rnorm(n, mean = 0, sd = 2)
    (n-1) * var(x) / qchisq(alpha, df = n-1)
})
## count the number of intervals that contain sigma^2=4
sum(UCL > 4)
## or compute the mean to get the confidence level
mean(UCL > 4)
```

## R Note 7.1

- This is a nice way to replicate

## Example 7.6 (Robustness)

```{r 76}
n <- 20
alpha <- .05
UCL <- replicate(1000, expr = {
        x <- rchisq(n, df = 2)
        (n-1) * var(x) / qchisq(alpha, df = n-1)
    } )
sum(UCL > 4)
mean(UCL > 4)
```


# Closing / Ch. 7 Problem set

- Revisit HW.
- Questions?
- Students please summarize/ask questions about what we discussed today.
